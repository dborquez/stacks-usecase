# -*- coding: utf-8 -*-
"""segmentation_physionet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDhdupGHBOkZaSDy0PagzWoWaK4IW4-w
"""
import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import os
import pathlib
from pathlib import Path

import nibabel as nib
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split
import tensorflow as tf
import matplotlib.pyplot as plt


class ICHDataPreprocess:
    """preprocess ICH data as detailed in the paper.

    orig src: https://alpha.physionet.org/content/ct-ich/1.0.0/
    """

    def _resize(self, img: np.ndarray, size=151) -> np.ndarray:
        """resize `img` to `size`"""
        image = Image.fromarray(img.astype(np.int8), mode="L")
        return image.resize((size, size), resample=Image.BICUBIC)

    def _window_ct(self, ct_scan, w_level=40, w_width=120):
        """window ct scan and stack slices."""
        w_min = w_level - w_width / 2
        w_max = w_level + w_width / 2
        num_slices = ct_scan.shape[2]
        for s in range(num_slices):
            slice_s = ct_scan[:, :, s]
            slice_s = (slice_s - w_min) * (255 / (w_max - w_min))
            slice_s[slice_s < 0] = 0
            slice_s[slice_s > 255] = 255
            ct_scan[:, :, s] = slice_s
        return ct_scan

    def _mkdirp(self, path):
        pathlib.Path(path).mkdir(parents=True, exist_ok=True)

    def preprocess(self):
        """preprocess data.
        
        preprocess and save:
            windowed and stacked ct_scans as input for the model
            save segmented masks as labels for the model.
        """

        # raw data path , change to where the raw data is downloaded.
        base_path = "ICH-UNET-2020/"
        dir_name = "computed-tomography-images-for-intracranial-hemorrhage-detection-and-segmentation-1.3.1"
        base_data_dir = os.path.join(base_path, dir_name)

        # processed dataset path
        image_path = os.path.join(base_data_dir, "data", "images")
        label_path = os.path.join(base_data_dir, "data", "labels")

        if not os.path.isdir(image_path):
            self._mkdirp(image_path)
            self._mkdirp(label_path)

            # data params
            num_subjects = 82
            new_size = 512
            window_specs = [40, 120]  # brain window
            hemorrhage_diagnosis_df = pd.read_csv(
                Path(base_data_dir, "hemorrhage_diagnosis_raw_ct.csv")
            )
            hemorrhage_diagnosis_array = hemorrhage_diagnosis_df._get_values

            counter = 0
            for n in range(0 + 49, num_subjects + 49):
                if n > 58 and n < 66:  # no raw data were available for these subjects
                    next
                else:
                    # loading the CT scan
                    ct_dir_subj = pathlib.Path(
                        base_data_dir, "ct_scans", "{0:0=3d}.nii".format(n)
                    )
                    ct_scan_nifti = nib.load(str(ct_dir_subj))
                    ct_scan = ct_scan_nifti.get_data()
                    ct_scan = self._window_ct(ct_scan, window_specs[0], window_specs[1])

                    # loading the masks
                    masks_dir_subj = pathlib.Path(
                        base_data_dir, "masks", "{0:0=3d}.nii".format(n)
                    )
                    masks_nifti = nib.load(str(masks_dir_subj))
                    masks = masks_nifti.get_data()
                    idx = hemorrhage_diagnosis_array[:, 0] == n
                    n_slices = hemorrhage_diagnosis_array[idx, 1]
                    if n_slices.size != ct_scan.shape[2]:
                        print(
                            "Warning: the number of annotated slices does not equal the number of slices in NIFTI file!"
                        )

                    for ns in range(0, n_slices.size):
                        # saving slices (input)
                        x = ct_scan[:, :, ns]
                        x = self._resize(x, new_size)
                        x.save(pathlib.Path(image_path, str(counter) + ".png"))

                        # saving segmentation for slices (labels)
                        y = self._resize(masks[:, :, ns], new_size)
                        y.save(pathlib.Path(label_path, str(counter) + ".png"))
                        counter += 1
        return image_path, label_path

    def paint_4(self, img_paths: list) -> None:
        """iterates over file_paths and show first 4 images."""
        fig = plt.figure(figsize=(11, 6))
        for n, img_file in enumerate(img_paths):
            if n == 4:
                break
            img_raw = tf.io.read_file(img_file)
            img = tf.image.decode_image(img_raw)
            img = tf.squeeze(img)
            ax = fig.add_subplot(2, 2, n + 1)
            ax.imshow(img, cmap="bone")
            ax.set_title(os.path.basename(img_file), size=11)
        plt.tight_layout()
        plt.show()


class ICHDataGenerator:
    """generate tf.data.Dataset from preprocessed data.

    Take list of preprocessed dataset and labels, create
    training and testing sets, decode and return test and train data.
    """

    def __init__(self, dataset, labels):
        self.dataset, self.labels = pathlib.Path(dataset), pathlib.Path(labels)

    def _test_train_split(self, test_size=0.3):
        """splits and returns training and testing data along with their length."""
        # get the list of dataset and labels
        dataset_list = sorted([str(path) for path in self.dataset.glob("*.png")])
        labels_list = sorted([str(path) for path in self.labels.glob("*.png")])
        X_train, X_test, y_train, y_test = train_test_split(
            dataset_list, labels_list, test_size=test_size, random_state=26
        )
        train_img_label = tf.data.Dataset.from_tensor_slices((X_train, y_train))
        test_img_label = tf.data.Dataset.from_tensor_slices((X_test, y_test))
        return train_img_label, test_img_label, len(X_train), len(X_test)

    def _load_nd_decode(self, path):
        raw_img = tf.io.read_file(path)
        img = tf.image.decode_png(raw_img, channels=1)
        img = tf.cast(img, tf.float32) / 255.0
        return img

    def _load_nd_decode_labels(self, path):
        raw_img = tf.io.read_file(path)
        img = tf.image.decode_png(raw_img, channels=1)
        img = tf.cast(img, tf.float32)
        img //= 254
        return img

    def _load_data(self, x_path, y_path):
        """load input and label."""
        image = self._load_nd_decode(x_path)
        label = self._load_nd_decode_labels(y_path)
        return image, label

    def test_train_data(self):
        train_img_label, test_img_label, train_len, test_len = self._test_train_split()
        return (
            train_img_label.map(self._load_data),
            test_img_label.map(self._load_data),
            train_len,
            test_len,
        )


if __name__ == "__main__":
    """this should be done when the DNN model is called."""
    data_preprocess = ICHDataPreprocess()
    dataset, labels = data_preprocess.preprocess()
    data_gen = ICHDataGenerator(dataset, labels)

    BATCH_SIZE = 32
    BUFFER_SIZE = 1000

    train_img_label, test_img_label, train_length, _ = data_gen.test_train_data()
    train_dataset = (
        train_img_label.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
    )
    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    test_dataset = test_img_label.batch(BATCH_SIZE)
